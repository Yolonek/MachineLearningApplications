{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Building Convolutional Autoencoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfc61f7351931627"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-04T14:16:23.318784500Z",
     "start_time": "2025-03-04T14:16:21.682287200Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convolutional Encoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17af692d1a94207c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T14:16:23.334783800Z",
     "start_time": "2025-03-04T14:16:23.320284400Z"
    }
   },
   "id": "1341a7c64856f484",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ConvolutionalBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_shape : tuple[int, int, int],\n",
    "                 convolutional_filters: tuple[int:],\n",
    "                 convolutional_kernels: tuple[int:],\n",
    "                 convolutional_strides: tuple[int:]):\n",
    "        super(ConvolutionalBlock, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.convolutional_filters = convolutional_filters\n",
    "        self.convolutional_kernels = convolutional_kernels\n",
    "        self.convolutional_strides = convolutional_strides\n",
    "        self.num_conv_layers = len(self.convolutional_filters)\n",
    "        \n",
    "        self.conv_layers = self._build_convolutional_layers()\n",
    "    \n",
    "    def _build_convolutional_layers(self):\n",
    "        layers = []\n",
    "        in_channels = self.input_shape[0]\n",
    "        for i in range(self.num_conv_layers):\n",
    "            layers.append((\n",
    "                f'conv{i+1}',\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=self.convolutional_filters[i],\n",
    "                        kernel_size=self.convolutional_kernels[i],\n",
    "                        stride=self.convolutional_strides[i],\n",
    "                        padding=(self.convolutional_kernels[i] - 1) // 2\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm2d(self.convolutional_filters[i])\n",
    "                )\n",
    "            ))\n",
    "            in_channels = self.convolutional_filters[i]\n",
    "        return nn.Sequential(OrderedDict(layers))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "    \n",
    "    def summary(self):\n",
    "        print(self)\n",
    "        \n",
    "\n",
    "class ConvolutionalEncoder(ConvolutionalBlock):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_shape : tuple[int, int, int],\n",
    "                 convolutional_filters: tuple[int:],\n",
    "                 convolutional_kernels: tuple[int:],\n",
    "                 convolutional_strides: tuple[int:],\n",
    "                 latent_space_dimension: int = 2):\n",
    "        super(ConvolutionalEncoder, self).__init__(\n",
    "            input_shape=input_shape,\n",
    "            convolutional_filters=convolutional_filters,\n",
    "            convolutional_kernels=convolutional_kernels,\n",
    "            convolutional_strides=convolutional_strides\n",
    "        )\n",
    "        self.latent_space_dim = latent_space_dimension\n",
    "        self.shape_before_bottleneck = None\n",
    "        self.shape_flattened = None\n",
    "        \n",
    "        self.output_layer = self._build_output_layer()\n",
    "    \n",
    "    def _build_output_layer(self):\n",
    "        dummy_input = torch.zeros(1, *self.input_shape)\n",
    "        conv_out = self.conv_layers(dummy_input)\n",
    "        self.shape_before_bottleneck = conv_out.shape[1:]\n",
    "        self.shape_flattened = conv_out.numel()\n",
    "        return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=self.shape_flattened,\n",
    "                      out_features=self.latent_space_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.output_layer(self.conv_layers(x))\n",
    "    \n",
    "    def summary(self):\n",
    "        print(self)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T14:16:23.382784400Z",
     "start_time": "2025-03-04T14:16:23.342785400Z"
    }
   },
   "id": "70eed15a9ef65a25",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalEncoder(\n",
      "  (conv_layers): Sequential(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv4): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (output_layer): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=3136, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = [1, 28, 28]\n",
    "conv_filters = [32, 64, 64, 64]\n",
    "conv_kernels = [3, 3, 3, 3]\n",
    "conv_strides = [1, 2, 2, 1]\n",
    "latent_space_dim = 2\n",
    "encoder = ConvolutionalEncoder(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    convolutional_filters=conv_filters,\n",
    "    convolutional_kernels=conv_kernels,\n",
    "    convolutional_strides=conv_strides,\n",
    "    latent_space_dimension=latent_space_dim\n",
    ")\n",
    "encoder.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T14:16:23.435784100Z",
     "start_time": "2025-03-04T14:16:23.352287Z"
    }
   },
   "id": "8f7cb5f23dcb0881",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             320\n",
      "              ReLU-2           [-1, 32, 28, 28]               0\n",
      "       BatchNorm2d-3           [-1, 32, 28, 28]              64\n",
      "            Conv2d-4           [-1, 64, 14, 14]          18,496\n",
      "              ReLU-5           [-1, 64, 14, 14]               0\n",
      "       BatchNorm2d-6           [-1, 64, 14, 14]             128\n",
      "            Conv2d-7             [-1, 64, 7, 7]          36,928\n",
      "              ReLU-8             [-1, 64, 7, 7]               0\n",
      "       BatchNorm2d-9             [-1, 64, 7, 7]             128\n",
      "           Conv2d-10             [-1, 64, 7, 7]          36,928\n",
      "             ReLU-11             [-1, 64, 7, 7]               0\n",
      "      BatchNorm2d-12             [-1, 64, 7, 7]             128\n",
      "          Flatten-13                 [-1, 3136]               0\n",
      "           Linear-14                    [-1, 2]           6,274\n",
      "================================================================\n",
      "Total params: 99,394\n",
      "Trainable params: 99,394\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.03\n",
      "Params size (MB): 0.38\n",
      "Estimated Total Size (MB): 1.41\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(encoder, input_size=tuple(INPUT_SHAPE), device='cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T14:16:23.439785400Z",
     "start_time": "2025-03-04T14:16:23.384286100Z"
    }
   },
   "id": "7e2ca49d59f0664f",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.1759, -0.3644],\n        [ 1.6390, -0.6403],\n        [ 0.0442, -0.0863]], grad_fn=<AddmmBackward0>)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor = torch.randn(3, 1, 28, 28)\n",
    "encoder_output = encoder(test_tensor)\n",
    "encoder_output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T14:16:23.511785200Z",
     "start_time": "2025-03-04T14:16:23.398784300Z"
    }
   },
   "id": "5c0369c4f6993a9a",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([64, 7, 7])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.shape_before_bottleneck"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T14:16:23.513285Z",
     "start_time": "2025-03-04T14:16:23.413786500Z"
    }
   },
   "id": "4c4011229c0fa0e3",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now build a decoder using convolution transpose"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2dcd6dcf55af181"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ConvolutionalTransposeBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 shape_before_bottleneck: tuple[int:],\n",
    "                 convolutional_transpose_filters: tuple[int:],\n",
    "                 convolutional_transpose_kernels: tuple[int:],\n",
    "                 convolutional_transpose_strides: tuple[int:]):\n",
    "        super(ConvolutionalTransposeBlock, self).__init__()\n",
    "        self.shape_before_bottleneck = shape_before_bottleneck\n",
    "        self.convolutional_transpose_filters = convolutional_transpose_filters\n",
    "        self.convolutional_transpose_kernels = convolutional_transpose_kernels\n",
    "        self.convolutional_transpose_strides = convolutional_transpose_strides\n",
    "        self.num_convT_filters = len(convolutional_transpose_filters)\n",
    "        \n",
    "        self.convT_layers = self._build_convolutional_transpose_layers()\n",
    "    \n",
    "    def _build_convolutional_transpose_layers(self):\n",
    "        layers = []\n",
    "        in_channels = self.shape_before_bottleneck[0]\n",
    "        for i in range(self.num_convT_filters):\n",
    "            padding = (self.convolutional_transpose_kernels[i] - 1) // 2\n",
    "            output_padding = (self.convolutional_transpose_strides[i] -\n",
    "                              self.convolutional_transpose_kernels[i] + (2 * padding))\n",
    "            layers.append((\n",
    "                f'convTranspose{i+1}',\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=self.convolutional_transpose_filters[i],\n",
    "                        kernel_size=self.convolutional_transpose_kernels[i],\n",
    "                        stride=self.convolutional_transpose_strides[i],\n",
    "                        padding=padding,\n",
    "                        output_padding=output_padding\n",
    "                    ),\n",
    "                    nn.ReLU(),\n",
    "                    nn.BatchNorm2d(self.convolutional_transpose_filters[i])\n",
    "                )\n",
    "            ))\n",
    "            in_channels = self.convolutional_transpose_filters[i]\n",
    "        return nn.Sequential(OrderedDict(layers))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.convT_layers(x)\n",
    "    \n",
    "    def summary(self):\n",
    "        print(self)\n",
    "\n",
    "\n",
    "class ConvolutionalDecoder(ConvolutionalTransposeBlock):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 latent_space_dimension: int,\n",
    "                 shape_before_bottleneck: tuple[int:],\n",
    "                 convolutional_transpose_filters: tuple[int:],\n",
    "                 convolutional_transpose_kernels: tuple[int:],\n",
    "                 convolutional_transpose_strides: tuple[int:],\n",
    "                 out_channels: int = 3):\n",
    "        super(ConvolutionalDecoder, self).__init__(\n",
    "            shape_before_bottleneck=shape_before_bottleneck,\n",
    "            convolutional_transpose_filters=convolutional_transpose_filters,\n",
    "            convolutional_transpose_kernels=convolutional_transpose_kernels,\n",
    "            convolutional_transpose_strides=convolutional_transpose_strides\n",
    "        )\n",
    "        self.latent_space_dimension = latent_space_dimension\n",
    "        self.shape_before_bottleneck = shape_before_bottleneck\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.dense_layer = self._build_dense_layer()\n",
    "        self.output_layer = self._build_output_layer()\n",
    "    \n",
    "    def _build_dense_layer(self):\n",
    "        flattened_size = self.shape_before_bottleneck[0] * \\\n",
    "                         self.shape_before_bottleneck[1] * \\\n",
    "                         self.shape_before_bottleneck[2]\n",
    "        return nn.Linear(in_features=self.latent_space_dimension,\n",
    "                         out_features=flattened_size)\n",
    "    \n",
    "    def _build_output_layer(self):\n",
    "        padding = (self.convolutional_transpose_kernels[-1] - 1) // 2\n",
    "        output_padding = (self.convolutional_transpose_strides[-1] - \n",
    "                          self.convolutional_transpose_kernels[-1] + (2 * padding))\n",
    "        output_convT = nn.ConvTranspose2d(\n",
    "            in_channels=self.convolutional_transpose_filters[-1],\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.convolutional_transpose_kernels[-1],\n",
    "            stride=self.convolutional_transpose_strides[-1],\n",
    "            padding=padding,\n",
    "            output_padding=output_padding\n",
    "        )\n",
    "        return nn.Sequential(output_convT, nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.output_layer(self.convT_layers(\n",
    "            self.dense_layer(x).view(x.size(0), *self.shape_before_bottleneck)))\n",
    "    \n",
    "    def summary(self):\n",
    "        print(self)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T14:16:23.570284500Z",
     "start_time": "2025-03-04T14:16:23.427784800Z"
    }
   },
   "id": "16beb2c7aa163395",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalDecoder(\n",
      "  (convT_layers): Sequential(\n",
      "    (convTranspose1): Sequential(\n",
      "      (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (convTranspose2): Sequential(\n",
      "      (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (convTranspose3): Sequential(\n",
      "      (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (convTranspose4): Sequential(\n",
      "      (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (dense_layer): Linear(in_features=2, out_features=3136, bias=True)\n",
      "  (output_layer): Sequential(\n",
      "    (0): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder = ConvolutionalDecoder(\n",
    "    latent_space_dimension=latent_space_dim,\n",
    "    shape_before_bottleneck=encoder.shape_before_bottleneck,\n",
    "    convolutional_transpose_filters=conv_filters[::-1],\n",
    "    convolutional_transpose_kernels=conv_kernels[::-1],\n",
    "    convolutional_transpose_strides=conv_strides[::-1],\n",
    "    out_channels=INPUT_SHAPE[0]\n",
    ")\n",
    "decoder.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T14:16:23.585285800Z",
     "start_time": "2025-03-04T14:16:23.460286800Z"
    }
   },
   "id": "ac3e33c770426aef",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 3136]           9,408\n",
      "   ConvTranspose2d-2             [-1, 64, 7, 7]          36,928\n",
      "              ReLU-3             [-1, 64, 7, 7]               0\n",
      "       BatchNorm2d-4             [-1, 64, 7, 7]             128\n",
      "   ConvTranspose2d-5           [-1, 64, 14, 14]          36,928\n",
      "              ReLU-6           [-1, 64, 14, 14]               0\n",
      "       BatchNorm2d-7           [-1, 64, 14, 14]             128\n",
      "   ConvTranspose2d-8           [-1, 64, 28, 28]          36,928\n",
      "              ReLU-9           [-1, 64, 28, 28]               0\n",
      "      BatchNorm2d-10           [-1, 64, 28, 28]             128\n",
      "  ConvTranspose2d-11           [-1, 32, 28, 28]          18,464\n",
      "             ReLU-12           [-1, 32, 28, 28]               0\n",
      "      BatchNorm2d-13           [-1, 32, 28, 28]              64\n",
      "  ConvTranspose2d-14            [-1, 1, 28, 28]             289\n",
      "          Sigmoid-15            [-1, 1, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 139,393\n",
      "Trainable params: 139,393\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 2.12\n",
      "Params size (MB): 0.53\n",
      "Estimated Total Size (MB): 2.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(decoder, input_size=encoder_output.shape[1:], device='cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T14:16:23.630784300Z",
     "start_time": "2025-03-04T14:16:23.475785500Z"
    }
   },
   "id": "abe7d03b72061180",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 1, 28, 28])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output = decoder(encoder_output)\n",
    "decoder_output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T14:16:23.635785100Z",
     "start_time": "2025-03-04T14:16:23.506285200Z"
    }
   },
   "id": "f47394677598d157",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Autoencoder"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "601c6b85d861b253"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now construct an Autoencoder based on the following formula"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2b1cf8526523fb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ConvolutionalAutoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_shape : tuple[int, int, int],\n",
    "                 convolutional_filters: tuple[int:],\n",
    "                 convolutional_kernels: tuple[int:],\n",
    "                 convolutional_strides: tuple[int:],\n",
    "                 latent_space_dimension: int = 2,\n",
    "                 convolutional_transpose_filters: tuple[int:] = None,\n",
    "                 convolutional_transpose_kernels: tuple[int:] = None,\n",
    "                 convolutional_transpose_strides: tuple[int:] = None):\n",
    "        super(ConvolutionalAutoencoder, self).__init__()\n",
    "        \n",
    "        if convolutional_transpose_filters is None:\n",
    "            convolutional_transpose_filters = convolutional_filters[::-1]\n",
    "        if convolutional_transpose_kernels is None:\n",
    "            convolutional_transpose_kernels = convolutional_kernels[::-1]\n",
    "        if convolutional_transpose_strides is None:\n",
    "            convolutional_transpose_strides = convolutional_strides[::-1]\n",
    "        \n",
    "        self.encoder = ConvolutionalEncoder(\n",
    "            input_shape=input_shape,\n",
    "            convolutional_filters=convolutional_filters,\n",
    "            convolutional_kernels=convolutional_kernels,\n",
    "            convolutional_strides=convolutional_strides,\n",
    "            latent_space_dimension=latent_space_dimension\n",
    "        )\n",
    "        self.decoder = ConvolutionalDecoder(\n",
    "            latent_space_dimension=latent_space_dimension,\n",
    "            shape_before_bottleneck=self.encoder.shape_before_bottleneck,\n",
    "            convolutional_transpose_filters=convolutional_transpose_filters,\n",
    "            convolutional_transpose_kernels=convolutional_transpose_kernels,\n",
    "            convolutional_transpose_strides=convolutional_transpose_strides,\n",
    "            out_channels=input_shape[0]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.encoder(x))\n",
    "    \n",
    "    def summary(self):\n",
    "        print(self)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T14:16:23.653286Z",
     "start_time": "2025-03-04T14:16:23.542784700Z"
    }
   },
   "id": "f72eed4fbfbe5cb4",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvolutionalAutoencoder(\n",
      "  (encoder): ConvolutionalEncoder(\n",
      "    (conv_layers): Sequential(\n",
      "      (conv1): Sequential(\n",
      "        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv3): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (conv4): Sequential(\n",
      "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (output_layer): Sequential(\n",
      "      (0): Flatten(start_dim=1, end_dim=-1)\n",
      "      (1): Linear(in_features=3136, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): ConvolutionalDecoder(\n",
      "    (convT_layers): Sequential(\n",
      "      (convTranspose1): Sequential(\n",
      "        (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (convTranspose2): Sequential(\n",
      "        (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (convTranspose3): Sequential(\n",
      "        (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (convTranspose4): Sequential(\n",
      "        (0): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (dense_layer): Linear(in_features=3, out_features=3136, bias=True)\n",
      "    (output_layer): Sequential(\n",
      "      (0): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): Sigmoid()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "autoencoder = ConvolutionalAutoencoder(\n",
    "    input_shape=INPUT_SHAPE,\n",
    "    convolutional_filters=conv_filters,\n",
    "    convolutional_kernels=conv_kernels,\n",
    "    convolutional_strides=conv_strides,\n",
    "    latent_space_dimension=3\n",
    ")\n",
    "autoencoder.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T14:16:23.709783600Z",
     "start_time": "2025-03-04T14:16:23.553285500Z"
    }
   },
   "id": "b3fbcfa6abd56f30",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 28, 28]             320\n",
      "              ReLU-2           [-1, 32, 28, 28]               0\n",
      "       BatchNorm2d-3           [-1, 32, 28, 28]              64\n",
      "            Conv2d-4           [-1, 64, 14, 14]          18,496\n",
      "              ReLU-5           [-1, 64, 14, 14]               0\n",
      "       BatchNorm2d-6           [-1, 64, 14, 14]             128\n",
      "            Conv2d-7             [-1, 64, 7, 7]          36,928\n",
      "              ReLU-8             [-1, 64, 7, 7]               0\n",
      "       BatchNorm2d-9             [-1, 64, 7, 7]             128\n",
      "           Conv2d-10             [-1, 64, 7, 7]          36,928\n",
      "             ReLU-11             [-1, 64, 7, 7]               0\n",
      "      BatchNorm2d-12             [-1, 64, 7, 7]             128\n",
      "          Flatten-13                 [-1, 3136]               0\n",
      "           Linear-14                    [-1, 3]           9,411\n",
      "ConvolutionalEncoder-15                    [-1, 3]               0\n",
      "           Linear-16                 [-1, 3136]          12,544\n",
      "  ConvTranspose2d-17             [-1, 64, 7, 7]          36,928\n",
      "             ReLU-18             [-1, 64, 7, 7]               0\n",
      "      BatchNorm2d-19             [-1, 64, 7, 7]             128\n",
      "  ConvTranspose2d-20           [-1, 64, 14, 14]          36,928\n",
      "             ReLU-21           [-1, 64, 14, 14]               0\n",
      "      BatchNorm2d-22           [-1, 64, 14, 14]             128\n",
      "  ConvTranspose2d-23           [-1, 64, 28, 28]          36,928\n",
      "             ReLU-24           [-1, 64, 28, 28]               0\n",
      "      BatchNorm2d-25           [-1, 64, 28, 28]             128\n",
      "  ConvTranspose2d-26           [-1, 32, 28, 28]          18,464\n",
      "             ReLU-27           [-1, 32, 28, 28]               0\n",
      "      BatchNorm2d-28           [-1, 32, 28, 28]              64\n",
      "  ConvTranspose2d-29            [-1, 1, 28, 28]             289\n",
      "          Sigmoid-30            [-1, 1, 28, 28]               0\n",
      "ConvolutionalDecoder-31            [-1, 1, 28, 28]               0\n",
      "================================================================\n",
      "Total params: 245,060\n",
      "Trainable params: 245,060\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3.15\n",
      "Params size (MB): 0.93\n",
      "Estimated Total Size (MB): 4.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(autoencoder, input_size=tuple(INPUT_SHAPE), device='cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T14:17:04.122772800Z",
     "start_time": "2025-03-04T14:17:04.067273600Z"
    }
   },
   "id": "38b8a4a2118acc2f",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 1, 28, 28])"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder_output = autoencoder(test_tensor)\n",
    "autoencoder_output.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-03-04T14:18:48.530602200Z",
     "start_time": "2025-03-04T14:18:48.490103700Z"
    }
   },
   "id": "4b1a3832fa446431",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "16e2f2b5f95a59aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
